国内重要的机器人研发机构：

* 京东X事业部：无人机、 配送机器人、无人仓库、无人配送站
* 紫光物联：全屋无线智能家居
* 图灵机器人：机器人OS
* 北京全息机器人研究院
* 比特新大陆:比特币加速器、AI加速器Sophon
* 依图科技
* 旷视科技
* 云从科技:机器视觉
* 商汤科技：机器视觉
* 地平线:
* 深鉴科技
* 深维科技

通过学习方式将机器学习算法进行分类，主要包括：监督式学习、非监督式学习、半监督式学习、强化学习。
1. 监督式学习
在监督式学习方式下，输入数据被称为训练数据，每组训练数据有一个明确的表示或结果（Label）；在建立预测模型的时候，监督学习建立一个学习过程，将预测结果与训练数据的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。常见应用场景如分类问题和回归问题。参见算法如逻辑回归（Logistic Regression）、反向传播神经网络（Back Propagation Neural Network）。
2. 非监督式学习
在非监督式学习中，数据并不被标识，学习模型为为了推断出数据的一些内在结构。常见应用场景有关联规则的学习以及聚类等。常见算法有Apriori算法、K-Means算法。
3. 半监督式学习
在半监督式学习方式下，输入数据部分被标识，部分没有被标识，模型首先需要学习数据的内在结构以便合理的组织数据来进行预测；算法包括一些对常用监督学习算法的延伸，算法首先试图对为标记数据进行建模，在此基础上对标记数据进行预测。算法例如图论推理算法(Graph Inference)、拉普拉斯支持向量机（Laplacian SVM）等。

深度学习是一种特殊的机器学习，通过学习将世界使用嵌套的概念层次来表示并实现巨大的功能和灵活性，其中每个概念都定义为与简单概念相关联，而更为抽象的表示则以较不抽象的方式来计算。当数据很少时，深度学习算法的性能并不好。这是因为深度学习算法需要大量的数据来完美地理解它。另一方面，在这种情况下，传统的机器学习算法使用制定的规则，性能会比较好。在机器学习中，大多数应用的特征都需要专家确定然后编码为一种数据类型。特征可以使像素值、形状、纹理、位置和方向。大多数机器学习算法的性能依赖于所提取的特征的准确度。深度学习尝试从数据中直接获取高等级的特征，这是深度学习与传统机器学习算法的主要的不同。

ImageNet的标准输入都是224x224的。

CNN只能单独的取处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的，例如语音处理。所以为了解决一些这样类似的问题，能够更好的处理序列的信息，RNN就诞生了。循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。权重矩阵 W就是隐藏层上一次的值作为这一次的输入的权重。
![](RNNexpand.jpeg)
![](RNNfomula.jpeg)
![](nonlinear.jpeg)
U-Net - 用于图像分割的神经网络架构,最初被设计用于生物医学图像分割.
WaveNet －google的语音合成神经网络

one hidden layer proved to be universal approximater.

ensemble learning: 集成学习

支付宝App中的xNN的模型压缩工具 (xqueeze) 在业务模型上实现了近50倍的压缩比.

二值神经网络的历史不短于使用单精度浮点运算的神经网络（以下简称单精度网络）。

正则化—用于克服过拟合问题。正则化过程中通过添加一个 L1（LASSO）或 L2（Ridge）规范到权重向量 w（通过给定算法学习到的参数）上以「惩罚」损失项：
L（损失函数）+λN（w）—这里的λ是正则项，N（w）是 L1 或 L2 规范。

归一化—数据归一化是将一个或多个属性缩放至 0 到 1 的范围的过程。当不知道数据分布或分布不是高斯分布（钟形曲线）（）的时候，归一化是很有用的，可加速学习过程。

模型优化器—优化器是一种搜索技术，用于更新模型的权重。

  ● SGD—随机梯度下降，支持动量算法。
  ● RMSprop—适应性学习率优化方法，由 Geoff Hinton 提出。
  ● Adam—适应性矩估计（Adam）并同样使用了适应性学习率。
  
  TensorRT 是连接神经网络框架和硬件（GPU）平台的桥梁，它的支持范围覆盖终端设备芯片到服务器级别的各种芯片。
  
  MATLAB的新组件 GPU Coder，能自动将深度学习模型代码转换为 NVIDIA GPU 的 CUDA 代码，GPU Coder 转换后的 CUDA 代码可以脱离 MATLAB 环境直接高效地执行推断。经 MATLAB 内部基准测试显示，GPU Coder 产生的 CUDA 代码，比 TensorFlow 的性能高 7 倍，比 Caffe2 的性能高 4.5 倍。
  
机器学习：分类、聚合、回归

tensorflow是一个使用数据流图进行数值计算的开源软件库。

池化的重要原因之一是，一旦我们知道给定特征在一个给定的输入区域，我们可以忽略特征的确切位置将数据普遍化，减少过拟合。

AlphaGo Zero的成功表明算法比数据和计算更重要。

在无监督学习上我们取得的突破还很少,因此目前的深度学习都需要大量的训练数据。

反向传播并不是自然界生物大脑中存在的机制。


13）批次（Batches）——在训练神经网络的同时，不用一次发送整个输入，我们将输入分成几个随机大小相等的块。与整个数据集一次性馈送到网络时建立的模型相比，批量训练数据使得模型更加广义化。
14）周期（Epochs）——周期被定义为向前和向后传播中所有批次的单次训练迭代。这意味着1个周期是整个输入数据的单次向前和向后传递。你可以选择你用来训练网络的周期数量，更多的周期将显示出更高的网络准确性，然而，网络融合也需要更长的时间。另外，你必须注意，如果周期数太高，网络可能会过度拟合。
15）丢弃（Dropout）——Dropout是一种正则化技术，可防止网络过度拟合套。顾名思义，在训练期间，隐藏层中的一定数量的神经元被随机地丢弃。这意味着训练发生在神经网络的不同组合的神经网络的几个架构上。你可以将Dropout视为一种综合技术，然后将多个网络的输出用于产生最终输出。
16）批量归一化（Batch Normalization）——作为一个概念，批量归一化可以被认为是我们在河流中设定为特定检查点的水坝。这样做是为了确保数据的分发与希望获得的下一层相同。当我们训练神经网络时，权重在梯度下降的每个步骤之后都会改变，这会改变数据的形状如何发送到下一层。但是下一层预期分布类似于之前所看到的分布。 所以我们在将数据发送到下一层之前明确规范化数据。
19）池化（Pooling）——通常在卷积层之间定期引入池层。这基本上是为了减少一些参数，并防止过度拟合。最常见的池化类型是使用MAX操作的滤波器尺寸（2,2）的池层。它会做的是，它将占用原始图像的每个4 * 4矩阵的最大值。你还可以使用其他操作（如平均池）进行池化，但是最大池数量在实践中表现更好。
21）数据增强（Data Augmentation）——数据增强是指从给定数据导出的新数据的添加，这可能被证明对预测有益。例如，如果你使光线变亮，可能更容易在较暗的图像中看到猫，或者例如，数字识别中的9可能会稍微倾斜或旋转。在这种情况下，旋转将解决问题并提高我们的模型的准确性。通过旋转或增亮，我们正在提高数据的质量。这被称为数据增强。
循环神经网络
22）循环神经元（Recurrent Neuron）——循环神经元是在T时间内将神经元的输出发送回给它。如果你看图，输出将返回输入t次。展开的神经元看起来像连接在一起的t个不同的神经元。这个神经元的基本优点是它给出了更广义的输出。
![](rnn.png)

23）循环神经网络（RNN）——循环神经网络特别用于顺序数据，其中先前的输出用于预测下一个输出。在这种情况下，网络中有循环。隐藏神经元内的循环使他们能够存储有关前一个单词的信息一段时间，以便能够预测输出。隐藏层的输出在t时间戳内再次发送到隐藏层。展开的神经元看起来像上图。只有在完成所有的时间戳后，循环神经元的输出才能进入下一层。发送的输出更广泛，以前的信息保留的时间也较长。然后根据展开的网络将错误反向传播以更新权重。这被称为通过时间的反向传播（BPTT）。
25）激增梯度问题（Exploding Gradient Problem）——这与消失的梯度问题完全相反，激活函数的梯度过大。在反向传播期间，它使特定节点的权重相对于其他节点的权重非常高，这使得它们不重要。这可以通过剪切梯度来轻松解决，使其不超过一定值。

软件1.0（Software 1.0）是由程序员编写的对计算机的明确指令组成。通过编写每行代码，程序员可以确定程序空间中的某个特定点。Software 2.0 是用神经网络权重编写的。没有人参与这段代码的编写过程，因为权重有很多。

在损失函数C的表达式中，我们可以把C表达式看做是所有w参数的函数，也就是求这个多元函数的最值问题.那么成功的将一个神经网络的问题引入到数学中最优化的路上了.

曲线上的一点，其切线只有一条。但是曲面的一点，切线有无数条。而我们所说的偏导数就是指的是多元函数沿坐标轴的变化率.偏导数对应的几何意义: $\frac{\partial f}{\partial x_i}$是函数曲面f被曲面$x_i=0$所截得的曲线在某一点的切线的斜率。把这个函数变化最大的方向命名为梯度。

国内机器人本体要发展的好，在“伺服系统”、“控制器”、“核心算法”、“精密减速器”，以及“应用和集成技术”这五大领域至少要有2-3个是擅长的。这五大核心技术被称为机器人本体的“成功五要素”。首先伺服系统和控制器这两块要吃透，然后在核心算法方面需要做到比较好。

人工智能三个阶段：
１.运算智能：能存会算
２.感知智能：能听会说，能看会认
3.认知智能：能理解会思考

认知智能研究的核心问题：语言理解、知识表示、联想推理、自主学习

深度神经网络与大数据的结合成为当前人工智能的主流路径

但当语音进入我们的耳朵时，耳朵里的纤毛会根据它的长度不同与语音中不同的频率进行共振，

BP 算法属于优化理论中的梯度下降法（Gradient Descend）。将误差 e 作为全部权值和全部偏置值的函数。算法的目的是在自变量空间内找到 error 的全局极小点。“反向传播”名称的由来——“局部误差”或“灵敏度” delta 沿着反向向前传，逐层计算所有权值和偏置值的偏导数，最终得到梯度。

一个卷积层可以有多个滤波器，每一个叫做一个 channel 。

NNVM的可行性恰恰证明了现行的各大框架底层的重复性，而上层的多样性只是一个幌子。

NNVM和Intel曝光的Nervana，都分离了前后端。后端的独立，不仅减少了编译工作，最大的优势在于降低了传统框架做跨设备计算的代码耦合度。

池化单元接收传入的连接，并决定哪个连接能通过。在连续的卷积层之间一般会周期性地插入一个池化层，用于逐渐降低数据的空间尺寸，从而减少参数的数量，使得计算资源耗费变少，也能有效控制过拟合。内插单元执行相反的操作：它们接收一些信息并将其映射到更多信息。内插单元不是池化单元的唯一反向操作，但它们相对较为常见，因为它们实现起来很简单。二者在连接上分别像卷积和去卷积单元。

长短期记忆单元被用来应对循环单元中信息快速损失的问题。 LSTM单元是逻辑循环，从如何为计算机设计存储单元复制。与存储两种状态的RNN单元相比，LSTM单元存储四个：输出的当前和最后值以及“存储单元”状态的当前和最后值。它们有三个“门”：输入，输出，忘记，它们也只有常规的输入。这些门中的每一个都具有自己的重量，意味着连接到这种类型的单元需要设置四个权重（而不是仅一个）。门的功能很像流闸，而不是栅栏门：可以让一切都通过，也可以只是一点点信息通过，或者不让任何东西流过。这可以通过将输入信息乘以0到1的值来实现，该值存储在该门限值中。

门循环单元是LSTM单元的变异。它们也使用门来消除信息丢失，但是只需要两个门：更新和重启。这使得他们的表现力稍逊一筹，但也稍微快一些，因为他们使用的连接数量更少。实质上，LSTM单元和GRU单元之间存在两个区别：GRU单元不具有由输出门保护的隐藏单元状态，并且它们将输入和忘记门组合成单个更新门。这个思路是，如果你想允许很多新的信息，你可能会忘记一些旧的信息 。

成立一年半的深鉴科技开始也没想到，自主研发的DNNDK一不小心就对标了英伟达的TensorRT。深鉴研发了面向深度学习应用的可编程通用计算平台：亚里士多德架构DPU，定义和实现了一套用于映射DNN算法的高度优化可编程指令集。

生成对抗网络的要点是：有两个模型，一个是生成模型（generative model），一个是判别模型(discriminative model)。判别模型的任务是判断给定的图像看起来是自然的还是人为伪造的（图像来源于数据集）。生成模型的任务是生成看起来自然真实的、和原始数据相似的图像。生成器（generator）试图欺骗判别器（discriminator），判别器则努力不被生成器欺骗。模型经过交替优化训练，两种模型都能得到提升，直到到达一个“假冒产品和真实产品无法区分”的点。对抗网络最重要的用法之一是经过充分训练生成器之后生成看起来自然的图像。生成器在生成数据和人脸时效果很好，但使用CIFAR-10数据集时，生成的图像就十分模糊。

前馈神经网络在每一层都有感知机，会将输入的信息传递到下一层，网络的最后一层是输出。在给定的一层，节点之间不会直接相连。没有原始输入也没有输出的层就是隐藏层。前馈神经网络的目标与使用反向传播的其他监督神经网络很类似，让输入有理想的、经过训练的输出。前馈神经网络是解决分类和回归问题的一些最简单有效的神经网络。

SVM 背后的基本概念是，对于二元可分的模式（pattern），存在一个最优的超平面。对于不可二元分类的数据，我们可以使用核函数将原始数据转换到一个新的平面上。SVM 将区分超平面的边缘区域最大化。SVM 尤其适用于高纬空间，如果维度比样本数量大也依然有效。

机器学习常用算法一览：Growth,LDA,逻辑回归、RF、Word2Vector,GBDT,HMM、CRF、深度学习、推荐算法

词向量表达是NLP最核心问题，是其他NLP任务的基础:
１. 离散表达方式：One-hot representation,缺点：高维度；将单词完全孤立开，忽视单词间的联系
2. 分布式表达方法: word embedding, 将高维离散的one-hot向量投影到低维连续向量空间，语义相似的单词在embedding空间分布更接近