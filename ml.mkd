国内重要的机器人研发机构：

* 京东X事业部：无人机、 配送机器人、无人仓库、无人配送站
* 紫光物联：全屋无线智能家居
* 图灵机器人：机器人OS
* 北京全息机器人研究院
* 比特新大陆:比特币加速器、AI加速器Sophon
* 依图科技
* 旷视科技
* 云从科技:机器视觉
* 商汤科技：机器视觉
* 地平线:
* 深鉴科技
* 深维科技
* 深醒科技
* 阿里达摩院Ali-NPU
* Atomic Rules：基于FPGA提供高速网络的设计服务
* Atman:语言智能
* Intel的Nervana神经网络芯片:基于flexpoint,
* intel的Movidius计算棒
* Intel的Nervana图编译器，面向神经网络的高层执行图
* Perrone Robotics:基于FPGA的自动驾驶汽车软件平台
* 波士顿动力
* 上海熠知电子科技有限公司
* Zilliz:GPU硬件加速数据库
* 德云数聚科技有限公司
* 菲数科技有限公司
* 奇点汽车：自动驾驶
* 中科院计算所控制计算实验室
* 禾赛科技:激光雷达
* 中星微人工智能芯片技术公司：星光一号
* 百炼智能：提供自动化”知识”内容创作服务的AI公司
* 美图云视觉技术部门:专注于文本、图像和视频等领域的视觉算法研发和平台构建
* 蓦然认知
* 西井科技
* 四维图新
* 森国科
* 高仙自主移动导航
* XNOR: 可以让计算机视觉、语音识别等AI技术在处理能力和内存都有限的设备上运行
* Kortiq: 基于FPGA的CNN加速器
* Omnitek: 基于FPGA的图像处理
* Plethora: 工业物联网机器学习算法和实时分析
* 天津赛百特有限公司：工业机器人
* Aaware公司：基于FPGA的语音定位与识别
* 疯景科技：基于FPGA的360度VR相机
* 厦门瑞为信息技术有限公司：基于FPGA的机器视觉
* Swift Navigation: 基于FPGA的GPS厘米级定位
* 元橡科技：基于FPGA的车载视觉传感器模块
* 深圳视显光电科技有限公司：基于FPGA的显示技术
* 上海知津信息科技有限公司：基于FPGA的机器人视觉定位系统
* 奇点机智：语音处理
* 云知声:物联网AI芯片及其解决方案雨燕
* 眼擎科技:视觉解决方案

传统目标检测方法大致分为如下三步：首先在给定的图像上选择一些候选的区域，然后对这些区域提取特征，最后使用训练的分类器进行分类。由于目标可能出现在图像的任何位置，而且目标的大小、长宽比例也不确定，所以需要采用滑动窗口的策略对整幅图像进行遍历，而且需要设置不同的尺度，不同的长宽比。特征提取常用的手段是SIFT和HOG。传统目标检测存在的两个主要问题：一个是基于滑动窗口的区域选择策略没有针对性，时间复杂度高，窗口冗余；二是手工设计的特征对于多样性的变化并没有很好的鲁棒性。 深度学习特别是CNN的出现使得传统目标检测方法的特征提取和分类可以合并在一起做。R-CNN是将图像切割为2000个左右候选区域，然后将每个区域绽放到227x227之后输入到CNN进行识别和分类。从R-CNN, Fast R-CNN, Faster R-CNN，流程变得越来越精简，精度越来越高，速度也越来越快。

SSD模型采用VGG16作为基础网络结构，利用astrous算法将fc6和fc7层转化为两个卷积层，再额外增加3个卷基层和一个平均池化层。

CNN的有效性依赖于图像的局域相关性、平移不变性和缩放不变性。

赫布理论:人脑里有很多神经元，他们可以接受、处理和互相传递信息，协同工作，人脑就是靠这强神经网络处理各种复杂的问题。

在神经网络中，给定一个网络结构，相当于是定义了一个函数集，这个函数集有大量参数来决定。确定了一组参数也就定义了一个函数。因此一个具体的网络是由结构和参数共同决定的。而决定神经网络的结构只能靠直觉来试错。

万能逼近定理:Given enough hidden neurons, any continous function $f:R^N->R^M$　can be realized by a network with one hidden layer.

a two layers of logic gates can represent any boolean function, but multiple layers of logic gates to build some functions are much simple.类似的，实际上神经网络不会是矮胖型的。

目前主流的目标检测算法主要是基于深度学习模型，可以分成两步法和一步法：两步法（two-stage）检测算法将检测问题划分为两个阶段，首先产生候选区域（region proposals），然后对候选区域分类（一般还需要对位置精修），这类算法的典型代表是基于region proposal的R-CNN系算法，如R-CNN，Fast R-CNN，Faster R-CNN等；一步法（one-stage）检测算法不需要region proposal阶段，直接产生物体的类别概率和位置坐标值，比较典型的算法如YOLO和SSD。目标检测模型的主要性能指标是检测准确度和速度，对于准确度，目标检测要考虑物体的定位准确性，而不单单是分类准确度。一般情况下，两步法（two-stage）算法在准确度上有优势，而一步法（one-stage）算法在速度上有优势。

自动驾驶包含十大技术，包括底层的工程类的偏支撑性的技术：硬件、车载系统、人机交互、智能互联以及系统安全，以及之上的汽车大脑、环境感知、地图定位、行为预测和规划控制。

高精度地图可以帮助我们做高精度定位。高精地图的误差一般在 10 厘米之内，所以无人车可以根据地图的一些自身定位技术知道自己在哪个车道线里面，以及离前方路口还有多远。

机器学习研究的是计算机怎样模拟人类的学习行为，以获取新的知识或技能，并重新组织已有的知识结构使之不断改善自身。简单一点说，就是计算机从数据中学习出规律和模式，以应用在新数据上做预测的任务。从功能的角度分类，机器学习在一定量级的数据上，可以解决下列问题：
1.分类问题：根据数据样本上抽取出的特征，判定其属于有限个类别中的哪一个。比如：垃圾邮件识别(结果类别：1、垃圾邮件 2、正常邮件)。
2.回归问题：根据数据样本上抽取出的特征，预测一个连续值的结果。比如：星爷《美人鱼》票房
3.聚类问题：根据数据样本上抽取出的特征，让样本抱抱团(相近/相关的样本在一团内)。比如：google的新闻分类。

激光雷达和雷达非常适合用于鉴别形状，但是不适合阅读标记，弄清色彩等，这些任务只能通过摄像机来实现。

CNN对于对象的姿态并不敏感，例如位置、大小、方向、变形、速度、反射率、色调和纹理等，因些需要为每一种情况添加训练数据。在CNN中通过大量增加训练数据或者增加可以泛化的最大池化层来处理，但是完全丢失了实际信息。

全连接层在整个卷积神经网络中起到“分类器”的作用，即通过卷积、激活函数、池化等深度网络后，再经过全连接层对结果进行识别分类。

通过学习方式将机器学习算法进行分类，主要包括：监督式学习、非监督式学习、半监督式学习、强化学习。
1. 监督式学习
在监督式学习方式下，输入数据被称为训练数据，每组训练数据有一个明确的表示或结果（Label）；在建立预测模型的时候，监督学习建立一个学习过程，将预测结果与训练数据的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。常见应用场景如分类问题和回归问题。参见算法如逻辑回归（Logistic Regression）、反向传播神经网络（Back Propagation Neural Network）。
2. 非监督式学习
在非监督式学习中，数据并不被标识，学习模型为为了推断出数据的一些内在结构。常见应用场景有关联规则的学习以及聚类等。常见算法有Apriori算法、K-Means算法。
3. 半监督式学习
在半监督式学习方式下，输入数据部分被标识，部分没有被标识，模型首先需要学习数据的内在结构以便合理的组织数据来进行预测；算法包括一些对常用监督学习算法的延伸，算法首先试图对为标记数据进行建模，在此基础上对标记数据进行预测。算法例如图论推理算法(Graph Inference)、拉普拉斯支持向量机（Laplacian SVM）等。

深度学习是一种特殊的机器学习，通过学习将世界使用嵌套的概念层次来表示并实现巨大的功能和灵活性，其中每个概念都定义为与简单概念相关联，而更为抽象的表示则以较不抽象的方式来计算。当数据很少时，深度学习算法的性能并不好。这是因为深度学习算法需要大量的数据来完美地理解它。另一方面，在这种情况下，传统的机器学习算法使用制定的规则，性能会比较好。在机器学习中，大多数应用的特征都需要专家确定然后编码为一种数据类型。特征可以使像素值、形状、纹理、位置和方向。大多数机器学习算法的性能依赖于所提取的特征的准确度。深度学习尝试从数据中直接获取高等级的特征，这是深度学习与传统机器学习算法的主要的不同。

ImageNet的标准输入都是224x224的。

CNN只能单独的取处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的，例如语音处理。所以为了解决一些这样类似的问题，能够更好的处理序列的信息，RNN就诞生了。循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。权重矩阵 W就是隐藏层上一次的值作为这一次的输入的权重。
![](RNNexpand.jpeg)
![](RNNfomula.jpeg)
![](nonlinear.jpeg)
U-Net - 用于图像分割的神经网络架构,最初被设计用于生物医学图像分割.
WaveNet －google的语音合成神经网络

one hidden layer proved to be universal approximater.

ensemble learning: 集成学习

支付宝App中的xNN的模型压缩工具 (xqueeze) 在业务模型上实现了近50倍的压缩比.

二值神经网络的历史不短于使用单精度浮点运算的神经网络（以下简称单精度网络）。

正则化—用于克服过拟合问题。正则化过程中通过添加一个 L1（LASSO）或 L2（Ridge）规范到权重向量 w（通过给定算法学习到的参数）上以「惩罚」损失项：
L（损失函数）+λN（w）—这里的λ是正则项，N（w）是 L1 或 L2 规范。

归一化—数据归一化是将一个或多个属性缩放至 0 到 1 的范围的过程。当不知道数据分布或分布不是高斯分布（钟形曲线）（）的时候，归一化是很有用的，可加速学习过程。

模型优化器—优化器是一种搜索技术，用于更新模型的权重。

  ● SGD—随机梯度下降，支持动量算法。
  ● RMSprop—适应性学习率优化方法，由 Geoff Hinton 提出。
  ● Adam—适应性矩估计（Adam）并同样使用了适应性学习率。
  
  TensorRT 是连接神经网络框架和硬件（GPU）平台的桥梁，它的支持范围覆盖终端设备芯片到服务器级别的各种芯片。
  
  MATLAB的新组件 GPU Coder，能自动将深度学习模型代码转换为 NVIDIA GPU 的 CUDA 代码，GPU Coder 转换后的 CUDA 代码可以脱离 MATLAB 环境直接高效地执行推断。经 MATLAB 内部基准测试显示，GPU Coder 产生的 CUDA 代码，比 TensorFlow 的性能高 7 倍，比 Caffe2 的性能高 4.5 倍。
  
机器学习：分类、聚合、回归

tensorflow是一个使用数据流图进行数值计算的开源软件库。

池化的重要原因之一是，一旦我们知道给定特征在一个给定的输入区域，我们可以忽略特征的确切位置将数据普遍化，减少过拟合。

AlphaGo Zero的成功表明算法比数据和计算更重要。

在无监督学习上我们取得的突破还很少,因此目前的深度学习都需要大量的训练数据。

反向传播并不是自然界生物大脑中存在的机制。


13）批次（Batches）——在训练神经网络的同时，不用一次发送整个输入，我们将输入分成几个随机大小相等的块。与整个数据集一次性馈送到网络时建立的模型相比，批量训练数据使得模型更加广义化。
14）周期（Epochs）——周期被定义为向前和向后传播中所有批次的单次训练迭代。这意味着1个周期是整个输入数据的单次向前和向后传递。你可以选择你用来训练网络的周期数量，更多的周期将显示出更高的网络准确性，然而，网络融合也需要更长的时间。另外，你必须注意，如果周期数太高，网络可能会过度拟合。
15）丢弃（Dropout）——Dropout是一种正则化技术，可防止网络过度拟合套。顾名思义，在训练期间，隐藏层中的一定数量的神经元被随机地丢弃。这意味着训练发生在神经网络的不同组合的神经网络的几个架构上。你可以将Dropout视为一种综合技术，然后将多个网络的输出用于产生最终输出。
16）批量归一化（Batch Normalization）——作为一个概念，批量归一化可以被认为是我们在河流中设定为特定检查点的水坝。这样做是为了确保数据的分发与希望获得的下一层相同。当我们训练神经网络时，权重在梯度下降的每个步骤之后都会改变，这会改变数据的形状如何发送到下一层。但是下一层预期分布类似于之前所看到的分布。 所以我们在将数据发送到下一层之前明确规范化数据。
19）池化（Pooling）——通常在卷积层之间定期引入池层。这基本上是为了减少一些参数，并防止过度拟合。最常见的池化类型是使用MAX操作的滤波器尺寸（2,2）的池层。它会做的是，它将占用原始图像的每个4 * 4矩阵的最大值。你还可以使用其他操作（如平均池）进行池化，但是最大池数量在实践中表现更好。
21）数据增强（Data Augmentation）——数据增强是指从给定数据导出的新数据的添加，这可能被证明对预测有益。例如，如果你使光线变亮，可能更容易在较暗的图像中看到猫，或者例如，数字识别中的9可能会稍微倾斜或旋转。在这种情况下，旋转将解决问题并提高我们的模型的准确性。通过旋转或增亮，我们正在提高数据的质量。这被称为数据增强。
循环神经网络
22）循环神经元（Recurrent Neuron）——循环神经元是在T时间内将神经元的输出发送回给它。如果你看图，输出将返回输入t次。展开的神经元看起来像连接在一起的t个不同的神经元。这个神经元的基本优点是它给出了更广义的输出。
![](rnn.png)

23）循环神经网络（RNN）——循环神经网络特别用于顺序数据，其中先前的输出用于预测下一个输出。在这种情况下，网络中有循环。隐藏神经元内的循环使他们能够存储有关前一个单词的信息一段时间，以便能够预测输出。隐藏层的输出在t时间戳内再次发送到隐藏层。展开的神经元看起来像上图。只有在完成所有的时间戳后，循环神经元的输出才能进入下一层。发送的输出更广泛，以前的信息保留的时间也较长。然后根据展开的网络将错误反向传播以更新权重。这被称为通过时间的反向传播（BPTT）。
25）激增梯度问题（Exploding Gradient Problem）——这与消失的梯度问题完全相反，激活函数的梯度过大。在反向传播期间，它使特定节点的权重相对于其他节点的权重非常高，这使得它们不重要。这可以通过剪切梯度来轻松解决，使其不超过一定值。

软件1.0（Software 1.0）是由程序员编写的对计算机的明确指令组成。通过编写每行代码，程序员可以确定程序空间中的某个特定点。Software 2.0 是用神经网络权重编写的。没有人参与这段代码的编写过程，因为权重有很多。

在损失函数C的表达式中，我们可以把C表达式看做是所有w参数的函数，也就是求这个多元函数的最值问题.那么成功的将一个神经网络的问题引入到数学中最优化的路上了.

曲线上的一点，其切线只有一条。但是曲面的一点，切线有无数条。而我们所说的偏导数就是指的是多元函数沿坐标轴的变化率.偏导数对应的几何意义: $\frac{\partial f}{\partial x_i}$是函数曲面f被曲面$x_i=0$所截得的曲线在某一点的切线的斜率。把这个函数变化最大的方向命名为梯度。

国内机器人本体要发展的好，在“伺服系统”、“控制器”、“核心算法”、“精密减速器”，以及“应用和集成技术”这五大领域至少要有2-3个是擅长的。这五大核心技术被称为机器人本体的“成功五要素”。首先伺服系统和控制器这两块要吃透，然后在核心算法方面需要做到比较好。

人工智能三个阶段：
１.运算智能：能存会算
２.感知智能：能听会说，能看会认
3.认知智能：能理解会思考

认知智能研究的核心问题：语言理解、知识表示、联想推理、自主学习

深度神经网络与大数据的结合成为当前人工智能的主流路径

但当语音进入我们的耳朵时，耳朵里的纤毛会根据它的长度不同与语音中不同的频率进行共振，

BP 算法属于优化理论中的梯度下降法（Gradient Descend）。将误差 e 作为全部权值和全部偏置值的函数。算法的目的是在自变量空间内找到 error 的全局极小点。“反向传播”名称的由来——“局部误差”或“灵敏度” delta 沿着反向向前传，逐层计算所有权值和偏置值的偏导数，最终得到梯度。

一个卷积层可以有多个滤波器，每一个叫做一个 channel 。

NNVM的可行性恰恰证明了现行的各大框架底层的重复性，而上层的多样性只是一个幌子。

NNVM和Intel曝光的Nervana，都分离了前后端。后端的独立，不仅减少了编译工作，最大的优势在于降低了传统框架做跨设备计算的代码耦合度。

池化单元接收传入的连接，并决定哪个连接能通过。在连续的卷积层之间一般会周期性地插入一个池化层，用于逐渐降低数据的空间尺寸，从而减少参数的数量，使得计算资源耗费变少，也能有效控制过拟合。内插单元执行相反的操作：它们接收一些信息并将其映射到更多信息。内插单元不是池化单元的唯一反向操作，但它们相对较为常见，因为它们实现起来很简单。二者在连接上分别像卷积和去卷积单元。

长短期记忆单元被用来应对循环单元中信息快速损失的问题。 LSTM单元是逻辑循环，从如何为计算机设计存储单元复制。与存储两种状态的RNN单元相比，LSTM单元存储四个：输出的当前和最后值以及“存储单元”状态的当前和最后值。它们有三个“门”：输入，输出，忘记，它们也只有常规的输入。这些门中的每一个都具有自己的重量，意味着连接到这种类型的单元需要设置四个权重（而不是仅一个）。门的功能很像流闸，而不是栅栏门：可以让一切都通过，也可以只是一点点信息通过，或者不让任何东西流过。这可以通过将输入信息乘以0到1的值来实现，该值存储在该门限值中。

门循环单元是LSTM单元的变异。它们也使用门来消除信息丢失，但是只需要两个门：更新和重启。这使得他们的表现力稍逊一筹，但也稍微快一些，因为他们使用的连接数量更少。实质上，LSTM单元和GRU单元之间存在两个区别：GRU单元不具有由输出门保护的隐藏单元状态，并且它们将输入和忘记门组合成单个更新门。这个思路是，如果你想允许很多新的信息，你可能会忘记一些旧的信息 。

成立一年半的深鉴科技开始也没想到，自主研发的DNNDK一不小心就对标了英伟达的TensorRT。深鉴研发了面向深度学习应用的可编程通用计算平台：亚里士多德架构DPU，定义和实现了一套用于映射DNN算法的高度优化可编程指令集。

生成对抗网络的要点是：有两个模型，一个是生成模型（generative model），一个是判别模型(discriminative model)。判别模型的任务是判断给定的图像看起来是自然的还是人为伪造的（图像来源于数据集）。生成模型的任务是生成看起来自然真实的、和原始数据相似的图像。生成器（generator）试图欺骗判别器（discriminator），判别器则努力不被生成器欺骗。模型经过交替优化训练，两种模型都能得到提升，直到到达一个“假冒产品和真实产品无法区分”的点。对抗网络最重要的用法之一是经过充分训练生成器之后生成看起来自然的图像。生成器在生成数据和人脸时效果很好，但使用CIFAR-10数据集时，生成的图像就十分模糊。

前馈神经网络在每一层都有感知机，会将输入的信息传递到下一层，网络的最后一层是输出。在给定的一层，节点之间不会直接相连。没有原始输入也没有输出的层就是隐藏层。前馈神经网络的目标与使用反向传播的其他监督神经网络很类似，让输入有理想的、经过训练的输出。前馈神经网络是解决分类和回归问题的一些最简单有效的神经网络。

SVM 背后的基本概念是，对于二元可分的模式（pattern），存在一个最优的超平面。对于不可二元分类的数据，我们可以使用核函数将原始数据转换到一个新的平面上。SVM 将区分超平面的边缘区域最大化。SVM 尤其适用于高纬空间，如果维度比样本数量大也依然有效。

机器学习常用算法一览：Growth,LDA,逻辑回归、RF、Word2Vector,GBDT,HMM、CRF、深度学习、推荐算法

词向量表达是NLP最核心问题，是其他NLP任务的基础:
１. 离散表达方式：One-hot representation,缺点：高维度；将单词完全孤立开，忽视单词间的联系
2. 分布式表达方法: word embedding, 将高维离散的one-hot向量投影到低维连续向量空间，语义相似的单词在embedding空间分布更接近

人脸检测是根据肤色等特征定位人脸区域，人脸识别是识别这个人到底是谁。而人脸检索是指给定一个或多个包含人脸的输入图像，从图像库中检索出包含输入图像中的人脸的那些图像。人脸检测和人脸检索通常都是非监督学习过程，而人脸识别是有监督学习的过程，需要使用一定数量的有标签的图像训练分类模型。当前比较经典的人脸数据集有LFW(Labeled Faces in the Wild)　FDDB(Face Detection Data set and Benchmark),前都主要用于人脸检测、人脸识别和人脸检索，而后者主要用于人脸检测和有脸检索。视频画面中的分辨率经常用多少P来表示，它表示一个视频画面的竖直像素数，例如1280x720就是720P。

色调和饱和库合称为色度，色度表示的是色彩的纯度。色度反映颜色的色调和饱和度，颜色由亮度和色度共同表示。人眼一般感到红光最暗，蓝光次之，而黄绿光最亮。色彩的饱和度就是颜色的纯净度，饱和度越高，色彩越高，色彩越鲜明。色彩的饱和度取决于颜色中白光的比例，色彩中的白光越多饱和度就越低，纯单色光的饱和度为１００%，纯白光的饱和度为0%.对比度是图像的黑白之间的差异，差异越大对比度越高，能表示的颜色就越丰富。图像的纹理、颜色和形状一起用来表述视觉特征，也就是人的视觉能感受到的自然特征。灰度分布在空间位置上的反复出现就形成了纹理，所以图像空间中一定距离的两个像素是有关系，灰度共生矩阵就是利用图像灰度级之间的相关性来描述纹理的一种常用方法。灰度共生矩阵的特征有:熵、ＡSＭ能量、对比度、均匀度和自相关。如果精确度共生矩阵元素值分布均匀，则表示图像近于随机，熵值就越大。ＡSＭ能量表示灰度共生矩阵中每个元素的平方和，表示图像是否分布均匀，纹理是否粗糙。图像越有结构，ASM就越大。对比度表示图像的清晰程度和纹理深浅程度。灰度共生矩阵中偏离对角线的元素值越大，对比度就越大，纹理也就越深。均匀度表示图像纹理的粗细度，纹理越粗均匀度越大；纹理越细均匀度越小。自相关描述灰度共生矩阵中水平或竖直方向上的相似度。如果矩阵中水平方向的元素值均匀相等，则水平方向的相关性大，对应的图像在水平方向上越有纹理，则水平方向灰度共生矩阵的自相关值大于其他方向灰度共生矩阵的自相关值。

图像的噪声可分为加性噪声、乘性噪声和量化噪声，常用的噪声模型有高斯噪声、椒盐噪声、均匀分布噪声、指数分布噪声和Gamma分布噪声。比较经典的去噪算法有均值滤波、自适应维纳滤波、中值滤波、形态学滤波和小波滤波。高斯噪声是一种源于电路噪声和由低光照或高温带来的传感器噪声，它的概率密度呈正态分布。椒盐噪声是一种只由两种灰度值作为噪声出现在图像上的噪声，因此也称为双极脉冲噪声，负脉冲时黑点出现，正脉冲时白点出现。

均值滤波是对图像中坐标为(x,y)的像素，以訪点为中心创建一个矩形框，计算訪矩形框中像素的平均值，用得到的平均值替换像素点(x,y)的值。

中值滤波是顺序统计滤波中的一种，顺序统计滤波中还有最小值滤波、最大值滤波。中值滤波适合去除椒盐噪声，因为它可以保留图像的边缘。对于图像上坐标为(x,y)的像素，以訪像素为中心的一个mxn的像素窗口，找出訪区域内中间的像素值，用于替换点(x,y)的像素值。

经典人脸识别算法包括DPM(Deformable Part Model),LAEO(Looking At Each Other),Viola&Jones. DPM使用可变形部分模型目标检测框架，包含一个训练好的、可以检测多角度人脸的模型对输入的检测图片在检测模型上进行处理后得到若干检测框，初步得到的检测框经过ＮMS(Non-Maximum Suppression,非极大值抑制的重要性)处理即可得到最终的人脸检测结果。NMS的目的是保证每个检测实例只有一个检测框。对于同一个检测实例，选择得分最高的检测框的同时，自动移除得分低的检测框。当两个检测框的重叠区域面向占两个检测框面积总和的比例大于某个值时则认为两个检测框不是一个检测实例。LAEO主要用于检测视频中的人是否正在看彼此，首先使用上身检测器检测出人身体的上半部分，再使用头部检测器检测出人头，最后再估计人头的姿态最终确定人们是否正在看彼此。ＬAＥO的人脸检测精度不如DPM,但是可以处理比较大的图像。Viola&Jones算法己经集成在OpenCV中，可用于快速的人脸识别。Viola&Jones检测器主要分为三部分：
１。使用积分通道快速计算图像的特征值
２。利用AdaBoost分类器筛选特征
３。将AdaBoost分类器改为级联分类器，从而快速丢弃非人脸特征。

机器学习，指的是机器使用大量数据集而非硬编码规则来进行学习的能力。监督学习指的是利用已标注数据集进行的学习，该数据中包含输入和期望输出。

全连接网络相邻两层的节点都有边相连，而卷积神经网络相邻节点只有部分节点相连。卷积神经网络可以减小参数个数的目的。和全连接神经网络相比，卷积神经网络的优势在于共享权重和稀疏连接,从而防止过度拟合。

ARM最近宣布推出其计算库ACL为ARM Cortex-A系列CPU处理器和ARM Mali系列GPU实现了软件功能的综合集成。

通过加入若干pool层，CNN中隐层的长和宽不断缩小。当长宽缩小到一定程度(通常是个位数)的时候，就可以应用全连接层了。

CNN 之所以能够有效，在于它利用了图像中的一些约束。特点1是图像的局域相关性(图像上右上角某点跟远处左下角某点关系不大)；特点2是图像的平移不变性(图像右上角的形状，移动到左下角仍然是那个形状)；特点3是图像的放缩不变性(图像缩放后，信息丢失的很少)。这些约束的加入能让识别过程变得可控，对训练数据的需求降低，更不容易出现过拟合。

所谓过拟合，就是深度网络只看过了少量的样本导致只能识别这小部分的图片，丧失了“泛化”（Generalization）能力，不能够识别其它没见过、但是也是相似的图片。

gpu以相对稍慢的速度运行，但是能够通过使用大量运算逻辑单元（ALU）来提供很大的并行度。

Arm的Trillium项目包括一款为移动设备而设计的机器学习处理器、一款目标检测处理器和一个神经网络软件库。目标检测处理器是基于 Arm 现有的 IP 族 Spirit 的迭代。Spirit 是主导 Hive 安防摄像头的目标检测加速器。Compute Library是arm的通常算法加速库。

权值衰减也可以防止过度拟合

RNN的核心特征是“循环”， 即系统的输出会保留在网络里， 和系统下一刻的输入一起共同决定下一刻的输出。

从CNN可视化的角度来讲，感受野（receptive field）就是输出featuremap某个节点的响应对应的输入图像的区域。可以使用堆积层，抽样、扩张等方法增加感受野。卷积核越大，receptive field（感受野）越大，看到的图片信息越多，因此获得的特征越好。虽说如此，但是大的卷积核会导致计算量的暴增，不利于模型深度的增加，计算性能也会降低。

Densenet每一层都包含来自前面所有层的附加输入，并且将自身的特征层传递到随后的所有层。该网络有一些不可抗拒的优势：它消除了梯度消失的问题，增强了特征传播，促进了特征重复使用，大大的减少了参数的数目。

卷积层在CNN中起到特征抽象和提取的作用，这也是CNN区别于传统的ANN或SVM的重要不同，在传统机器学习算法中，我需要人为的指定特征是什么，比如经典的HOG+SVM的行人检测方案，HOG就是一种特征提取方法。所以我们送入SVM分类器中的其实HOG提取出来的特征，而不是图片的本身，所以有时特征提取算法的选择会比分类模型更为重要。而在CNN中，特征提取的工作在卷积层自动完成了.从这个角度上再思考一步的话，其实CNN可以看做一个自学习的特征提取+softmax分类器。CNN中的卷积层操作与图像处理中的卷积是一样的，不同之处在于，在传统图像处理中，我们人为指定卷积核，比如Soble。而在CNN中，卷积核的尺寸是人为指定的，但是卷积核内的数全部都是需要不断学习得到的。池化操作（Pooling）用于特征融合和降维，其实也是一种类似卷积的操作，只是池化层的所有参数都是超参数，都是不用学习得到的。CNN中的全连接层与传统神经网络中的作用是一样的，负责逻辑推断，所有的参数都需要学习得到。另外，全连接层还有一个作用是去除空间信息（通道数），是一种将三维矩阵变成向量的过程（一种全卷积操作）。Softmax用于将神经元的输出变成概率的形式。

传统的卷积层层叠网络会遇到一个问题，当层数加深时，网络的表现越来越差，很大程度上的原因是因为当层数加深时，梯度消散得越来越严重，以至于反向传播很难训练到浅层的网络。

CNN主要用来识别位移、缩放及其他形式扭曲不变性的二维图形。

人工智能识别内容最大的难点在于攻克语义的复杂性，其涉及到对逻辑推理和因果关系的上下文分析。人工智能可以鉴别色情内容，但在区别色情、性感、艺术等照片上还存在很大难度。

Winograd算法适合为具备小型滤波器的CNN推导高效算法。

线性模型（linear model）试图通过属性的线性组合来进行预测。“线性回归”（linear regression）试图学得一个线性模型以尽可能准确的预测实际输出标记。均方误差有非常好的几何意义，它对应了常用的欧几里得距离或简称“欧式距离”（Euclidean distance）。基于均方误差最小化进行模型求解的方法称为“最小二乘法”（least square method）。在线性回归中，最小二乘法就是输入找到一条直线，使所有样本到直线上的欧式距离之和最小。“多元线性回归”（multivariate linear regression）也是利用最小二乘法来对w和b进行估计。然而，许多任务中我们会遇到大量的变量，其数目甚至超过样例数，导致可以解出多个w，他们都能使均方误差最小化。选择哪一个最为输出，将由学习算法的归纳偏好决定，常见的做饭是引入正则化（regularization）项。这样得到的模型称为“广义线性模型”（generalized linear model）。

不同于支持向量机或随机森林的决策框架，深层神经网络所做出的决定无法解释，而且不一定可靠。

SVM属于有监督学习模型，可用于模式识别、分类以及回归分析。

由简至繁SVM可分类为三类：线性可分（linear SVM in linearly separable case）的线性SVM、线性不可分的线性SVM、非线性（nonlinear）SVM。，实现经验风险和置信范围的最小化，从而达到在统计样本量较少的情况下，亦能获得良好统计规律的目的。线性SVM与逻辑回归紧张密相关。SVM是一个强大的工具，但是它对计算和存储需求随着训练样本的增加而急剧增长。SVM的核心是一个二次规划问题.其目的是找出一条最优的分类超平面W^Tx+b=0使得所有点离分类超平面的距离尽可能远，因为数据点离分界超平面的远近可以表示为分类预测的确信度;在几何上，W恰好与分类超平面正交，称为法向量。对于任意的数据点x,如果W^Tx+b>0则表示x在超平面上方，否则表示x在超平面下方。为了求出分类超平面的参数Ｗ和b，可以构造一个最优化问题，优化目标是\delta最大。经过推导会发现W和b的取值仅与边界上的数据点有关，这些点被称为支持向量。

支持向量机中核函数的价值在于它虽然也是讲特征进行从低维到高维的转换，但核函数绝就绝在它事先在低维上进行计算，而将实质上的分类效果表现在了高维上，也就上文所说的避免了直接在高维空间中的复杂计算。SVM一般只能用在二类问题，对于多类问题效果不好。

SVM主要思想有两点：
1.它是针对线性可分情况进行分析，对于线性可分的情况，通过使用非线性映射算法将低维输入空间线性不可分的样本转换为高级空间的线性可分。
2.它是基本结构风险最小化理论之上在特征空间中构造分割超平面，使得机器学习得到全局最优化，并且在整个样本空间的期望风险以某个概率满足一定的上界。

在使用线性分类器W^Tx+b=0进行二分类时，对任意数据点要得知道訪点属于哪一类，需要将预测值z=W^Tx+b映射到(0,1)区间，而且要求映射函数单调可微并接近阶跃函数，否则大量点映射到0-1之间会影响精度。常见的映射函数包括$y=\frac{1}{1+e^{-z}}$


逻辑回归中，直接将两个参数都初始化为零。而在神经网络中，通常将参数 w进行随机初始化，参数 b则初始化为 0 。除 w,b外的各种参数，如学习率 alpha、神经网络的层数 l，第 l 层包含的节点数 n^[k] 及隐藏层中用的哪种激活函数，都称为超参数（Hyper Parameters），因为它们的值决定了参数 w,b最后的值。

人脸关键点检测方法大致分为三种，分别是基 ASM(Active Shape Model)[1] 和 AAM (Active Appearnce Model)[2,3] 的传统方法；基于级联形状回归的方法 [4]；基于深度学习的方法 [5-10]。

TensorFlow是一个使用数据流图进行数值计算的开源软件库。

无监督学习有一个重要的问题是不知道它什么结束，无监督学习把领域扩张到了神经网络和深度学习里，出现了自编码器。自编码器和数据压缩算法背后的逻辑差不多，用一个子集来反映原始数据集的特征。



数据降维中的维是指数据的列数，两种常见的降维方法：主成份分析法和奇异值分解法，前者能够把数据集里的大多数变化联系起来的线性组合，后者把数据的矩阵分解成三个小矩阵。

有以下几种常用的聚类方法:k-means聚类（把所有数据点划分到k个互斥的组别里，复杂之处在于k的选择），层次聚类（把所有数据点划分到一些组别和它们的子级别里形成像话族谱一样的树状图），概率聚类（又称模糊k-mean，它把所有数据按照概率来划分，k-mean就是它的一个特例，k-mean的概率非０即１)。

ROS将现有的、通常难以使用、不兼容的传感器、执行器等硬件融合在一起，通过将其数据流转换成消息流水线使用在硬件驱动器和计算单元之间兼容的数据类型。它也是一组转换接口，用于运行多个外部的开源计算算法。

神经网络机器翻译就是一个典型的sequence to sequence模型，也就是一个encoder to decoder模型。

